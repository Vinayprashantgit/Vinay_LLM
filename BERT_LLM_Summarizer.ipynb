{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvFiHiqGMyZWLc2sg+Kf+3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-YaQbRv_8b2F"},"outputs":[],"source":["#Dependencies\n","pip install tensorflow transformers PyPDF2\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForQuestionAnswering, pipeline\n","import PyPDF2\n","import re\n","\n","def initialize_bert_model():\n","    # Load pre-trained BERT model and tokenizer\n","    model = TFBertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n","\n","    return model, tokenizer\n","\n","def extract_text_from_pdf(pdf_path):\n","    # Extract text from a PDF file using PyPDF2\n","    text = \"\"\n","    with open(pdf_path, 'rb') as file:\n","        pdf_reader = PyPDF2.PdfReader(file)\n","        for page_num in range(len(pdf_reader.pages)):\n","            text += pdf_reader.pages[page_num].extract_text()\n","    return text\n","\n","def chunk_text(text, max_chunk_size=512):\n","    # Split the text into chunks with a maximum size\n","    chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n","    return chunks\n","\n","def answer_question(document, question, model, tokenizer):\n","    # Tokenize inputs\n","    inputs = tokenizer(question, document, return_tensors=\"tf\", max_length=512, truncation=True)\n","\n","    # Get model output\n","    outputs = model(inputs)\n","\n","    # Extract start and end logits from model output\n","    start_logits = outputs.start_logits\n","    end_logits = outputs.end_logits\n","\n","    # Find the tokens with the highest probability as start and end positions\n","    start_idx = tf.argmax(start_logits, axis=1).numpy()[0]\n","    end_idx = tf.argmax(end_logits, axis=1).numpy()[0]\n","\n","    # Check if valid indices are found\n","    if 0 <= start_idx < len(inputs[\"input_ids\"].numpy()[0]) and 0 <= end_idx < len(inputs[\"input_ids\"].numpy()[0]):\n","        # Convert token indices to actual tokens\n","        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].numpy()[0])\n","\n","        # Get the answer span\n","        answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx + 1])\n","\n","        return answer\n","    else:\n","        return \"Answer not found\"\n","\n","def summarize_text(text):\n","    summarizer = pipeline(\"summarization\")\n","\n","    try:\n","        # Attempt to get the summary\n","        summary = summarizer(text, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","\n","        # Check if a valid summary is obtained\n","        if summary and 'summary_text' in summary[0]:\n","            # Split the summary into sentences\n","            sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', summary[0]['summary_text'])\n","\n","            # Join sentences with newline characters\n","            formatted_summary = '\\n'.join(sentences)\n","            return formatted_summary\n","        else:\n","            return \"Unable to generate a summary.\"\n","    except Exception as e:\n","        return f\"Error during summarization: {str(e)}\"\n","\n","\n","def main():\n","    # Get PDF path from user input\n","    pdf_path = \"/content/fundamentals_supplychain-1-4.pdf\"\n","\n","    # Extract text from the PDF\n","    document = extract_text_from_pdf(pdf_path)\n","\n","    # Chunk the document into smaller parts\n","    document_chunks = chunk_text(document)\n","\n","    # Get question from user input\n","    question = input(\"Ask a question: \")\n","\n","    # Initialize BERT model and tokenizer\n","    model, tokenizer = initialize_bert_model()\n","\n","    # Accumulate answers from each chunk\n","    all_answers = []\n","\n","    for i, chunk in enumerate(document_chunks):\n","        print(f\"Processing chunk {i + 1} of {len(document_chunks)}\")\n","        answer = answer_question(chunk, question, model, tokenizer)\n","        all_answers.append(answer)\n","\n","    # Combine answers from different chunks with new lines between them\n","    final_answer = '\\n'.join(all_answers)\n","\n","    # Summarize the final answer\n","    summarized_answer = summarize_text(final_answer)\n","    print(f\"Summarized Answer:\\n{summarized_answer}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"l7Ez3sQk8g_b"},"execution_count":null,"outputs":[]}]}